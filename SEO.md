# SEO y Control de Indexaci√≥n - Entersys

Este documento describe c√≥mo el sitio web de Entersys controla la indexaci√≥n por motores de b√∫squeda en diferentes ambientes.

## üéØ Objetivo

**Evitar que los ambientes de desarrollo y staging sean indexados por motores de b√∫squeda**, mientras se permite la indexaci√≥n completa en producci√≥n.

---

## üõ°Ô∏è Protecci√≥n Multi-Capa

### 1. Meta Tags en `<head>` del HTML

**Plugin**: `vite-plugins/html-meta-plugin.js`

Durante el build, se inyectan autom√°ticamente meta tags en el `<head>` del HTML base:

#### Development/Staging
```html
<meta name="robots" content="noindex, nofollow">
<meta name="googlebot" content="noindex, nofollow">
<meta name="environment" content="development">
```

#### Production
No se inyecta nada (permite indexaci√≥n natural).

---

### 2. Meta Tags en React (react-helmet-async)

**Componente**: `src/components/SEO/MetaTags.jsx`

El componente `MetaTags` detecta el ambiente din√°micamente y aplica la configuraci√≥n correcta:

```javascript
const robotsContent = config.app.isProd
  ? 'index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1'
  : 'noindex, nofollow';
```

**Ventajas**:
- SEO din√°mico por p√°gina
- Open Graph y Twitter Cards
- Schema.org structured data

---

### 3. Archivo `robots.txt`

**Plugin**: `vite-plugins/robots-plugin.js`

El plugin copia autom√°ticamente el archivo `robots.txt` correcto seg√∫n el ambiente:

| Ambiente | Archivo Origen | Comportamiento |
|----------|----------------|----------------|
| development | `robots.development.txt` | `Disallow: /` (bloquea TODO) |
| staging | `robots.staging.txt` | `Disallow: /` (bloquea TODO) |
| production | `robots.production.txt` | `Allow: /` (permite indexaci√≥n) |

#### Ejemplo: robots.development.txt
```txt
# Bloquear TODOS los robots
User-agent: *
Disallow: /

# Bloquear crawlers espec√≠ficos
User-agent: Googlebot
Disallow: /

User-agent: Bingbot
Disallow: /

# NO hay sitemap en desarrollo
```

#### Ejemplo: robots.production.txt
```txt
# Permitir acceso a todos los crawlers
User-agent: *
Allow: /

# Bloquear solo directorios sensibles
Disallow: /api/
Disallow: /*.json$

# Sitemap
Sitemap: https://entersys.mx/sitemap.xml
```

---

## üîß Configuraci√≥n T√©cnica

### Plugins de Vite

#### 1. `robots-plugin.js`
```javascript
import { robotsPlugin } from './vite-plugins/robots-plugin.js'

export default defineConfig({
  plugins: [robotsPlugin()],
})
```

**Funcionalidad**:
- Detecta el modo de Vite (`development`, `staging`, `production`)
- Copia el archivo `robots.{mode}.txt` a `robots.txt`
- Muestra logs informativos durante el build

**Logs de ejemplo**:
```
ü§ñ Robots Plugin: Modo detectado = development
‚úÖ Robots Plugin: robots.development.txt ‚Üí robots.txt
   # robots.txt para DESARROLLO - Entersys
‚ö†Ô∏è  Robots Plugin: INDEXACI√ìN BLOQUEADA (ambiente: development)
   Los motores de b√∫squeda NO indexar√°n este sitio.
```

#### 2. `html-meta-plugin.js`
```javascript
import { htmlMetaPlugin } from './vite-plugins/html-meta-plugin.js'

export default defineConfig({
  plugins: [htmlMetaPlugin()],
})
```

**Funcionalidad**:
- Modifica el `index.html` durante el build
- Inyecta meta tags `noindex, nofollow` en dev/staging
- No modifica nada en producci√≥n

**Logs de ejemplo**:
```
üè∑Ô∏è  HTML Meta Plugin: Inyectando meta tags para modo = development
‚ö†Ô∏è  HTML Meta Plugin: NOINDEX inyectado (ambiente: development)
   Meta tags agregados: robots, googlebot, environment
```

---

## ‚úÖ Verificaci√≥n

### 1. Verificar Meta Tags

Abrir DevTools y revisar el `<head>`:

**Development/Staging**:
```html
<head>
  <meta name="robots" content="noindex, nofollow">
  <meta name="googlebot" content="noindex, nofollow">
  <meta name="environment" content="development">
  ...
</head>
```

**Production**:
```html
<head>
  <meta name="robots" content="index, follow, max-snippet:-1, ...">
  ...
</head>
```

### 2. Verificar robots.txt

**Development**:
```bash
curl https://dev.entersys.mx/robots.txt
```
Debe mostrar:
```txt
User-agent: *
Disallow: /
```

**Production**:
```bash
curl https://entersys.mx/robots.txt
```
Debe mostrar:
```txt
User-agent: *
Allow: /
Sitemap: https://entersys.mx/sitemap.xml
```

### 3. Verificar en Google Search Console

**Development/Staging**:
- No deber√≠a aparecer en Search Console
- Si aparece, ser√° marcado como "Excluido por etiqueta 'noindex'"

**Production**:
- Debe aparecer indexado normalmente
- Sitemap debe ser procesado

---

## üöÄ Scripts NPM y SEO

| Script | Ambiente | Indexaci√≥n |
|--------|----------|------------|
| `npm run dev` | development | ‚ùå BLOQUEADA |
| `npm run dev:staging` | staging | ‚ùå BLOQUEADA |
| `npm run dev:prod` | production | ‚úÖ PERMITIDA |
| `npm run build:dev` | development | ‚ùå BLOQUEADA |
| `npm run build:staging` | staging | ‚ùå BLOQUEADA |
| `npm run build:prod` | production | ‚úÖ PERMITIDA |

---

## üõ†Ô∏è Troubleshooting

### Problema: Development est√° siendo indexado

**Soluci√≥n**:
1. Verificar que el build us√≥ el modo correcto:
   ```bash
   npm run build:dev  # NO npm run build
   ```
2. Verificar logs durante el build:
   ```
   ‚ö†Ô∏è  Robots Plugin: INDEXACI√ìN BLOQUEADA (ambiente: development)
   ```
3. Verificar robots.txt en el servidor:
   ```bash
   curl https://dev.entersys.mx/robots.txt
   ```
4. Verificar meta tags en el HTML:
   - Abrir DevTools ‚Üí Elements ‚Üí `<head>`
   - Buscar: `<meta name="robots" content="noindex, nofollow">`

### Problema: Production no est√° siendo indexado

**Soluci√≥n**:
1. Verificar que el build us√≥ modo production:
   ```bash
   npm run build:prod
   ```
2. Verificar logs durante el build:
   ```
   ‚úÖ Robots Plugin: INDEXACI√ìN PERMITIDA (producci√≥n)
   ```
3. Verificar robots.txt:
   ```bash
   curl https://entersys.mx/robots.txt
   # Debe mostrar: Allow: /
   ```
4. Enviar sitemap a Google Search Console

---

## üìä Cobertura de Crawlers Bloqueados

Los archivos `robots.development.txt` y `robots.staging.txt` bloquean expl√≠citamente:

### Search Engine Crawlers
- Googlebot
- Bingbot
- Slurp (Yahoo)
- DuckDuckBot
- Baiduspider
- YandexBot

### AI Crawlers
- OAI-SearchBot (OpenAI)
- ChatGPT-User
- PerplexityBot
- AndiBot
- ClaudeBot
- GoogleOtherBot
- Google-Extended
- CCBot
- anthropic-ai
- Claude-Web

---

## üîê Mejores Pr√°cticas

1. ‚úÖ **NUNCA** hacer deploy de staging/dev con `npm run build` (sin sufijo)
2. ‚úÖ **SIEMPRE** usar scripts espec√≠ficos: `build:dev`, `build:staging`, `build:prod`
3. ‚úÖ **VERIFICAR** los logs del build antes de hacer deploy
4. ‚úÖ **REVISAR** robots.txt despu√©s del deploy
5. ‚úÖ **MONITOREAR** Google Search Console para URLs indexadas incorrectamente

---

## üìö Referencias

- [Google: robots meta tag](https://developers.google.com/search/docs/crawling-indexing/robots-meta-tag)
- [Google: robots.txt](https://developers.google.com/search/docs/crawling-indexing/robots/intro)
- [Bing: robots meta tag](https://www.bing.com/webmasters/help/how-to-use-robots-meta-tag-6d4d02fa)
- [MDN: X-Robots-Tag HTTP header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Robots-Tag)

---

**√öltima actualizaci√≥n**: 2025-10-11
**Autor**: Equipo Entersys Dev
